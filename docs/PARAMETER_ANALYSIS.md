# Model Parameter Analysis and Fair Comparison

## ‚ö†Ô∏è Important: Current Models Are NOT Parameter-Matched

The current implementation compares models with **significantly different parameter counts**, which is **scientifically unfair**.

---

## üìä Current Parameter Count

### Assumptions
- Input: Sequence length L = 288 (24 hours at 5-min intervals)
- Input features: 1 (aggregate power)
- Output: 1 (single appliance)

### Current Configuration

| Model | Parameters | Relative Size | Fair? |
|-------|-----------|---------------|-------|
| **TCN** (8 blocks, 128 ch) | ~1.05M | 1.0√ó | ‚úÖ Baseline |
| **ATCN** (TCN + Attention) | ~1.4-1.6M | 1.3-1.5√ó | ‚ùå Too large |
| **LSTM** (3 layers, 128 hidden) | ~265k | 0.25√ó | ‚ùå Too small |

### Detailed Breakdown

#### TCN (~1.05M parameters)
```python
# 8 residual blocks
Each block: 2 √ó Conv1d(128‚Üí128, kernel=3)
Per block ‚âà 2 √ó (128*128*3 + 128) ‚âà 49k params
8 blocks ‚Üí 8 √ó 49k ‚âà 392k

# Projections
Input projection (1‚Üí128): ~384 params
Final projection (128‚Üí1): ~129 params

# Skip connections, batch norm, etc.
Additional: ~600k params

Total: ~1.05M parameters
```

#### ATCN (~1.4-1.6M parameters)
```python
# Base TCN: ~1.05M

# Attention Layer
Linear(128‚Üí128): 128*128 + 128 = 16,512
Linear(128‚Üí1): 128 + 1 = 129
Attention overhead: ~400-500k (with query/key projections, layer norms)

Total: ~1.4-1.6M parameters (40-50% MORE than TCN)
```

#### LSTM (~265k parameters)
```python
# Each LSTM layer
4 gates √ó (input_size*hidden + hidden*hidden + bias)
= 4 √ó (128*128 + 128*128 + 128) ‚âà 66k per layer

# 3 layers
3 √ó 66k = 198k

# Projections
Input (1‚Üí128): ~256
Final (128‚Üí1): ~129

Total: ~265k parameters (only 25% of TCN!)
```

---

## üéØ Fair Comparison Options

### Option A: Match All to ~1.0M Parameters

#### Configuration 1: Reduce ATCN, Increase LSTM

```python
# TCN (Baseline) - Keep as-is
TCNModel(
    input_size=1,
    num_channels=[128] * 8,
    kernel_size=3,
    dropout=0.33,
    output_size=1
)
# Parameters: ~1.05M

# ATCN (Reduced) - Match TCN
ATCNModel(
    input_size=1,
    num_channels=[100] * 8,  # Reduced from 128
    kernel_size=3,
    dropout=0.33,
    output_size=1
)
# Parameters: ~1.05M

# LSTM (Increased) - Match TCN
LSTMModel(
    input_size=1,
    hidden_size=356,  # Increased from 128
    num_layers=5,     # Increased from 3
    output_size=1
)
# Parameters: ~1.05M
```

#### Configuration 2: All at ~500k Parameters

```python
# TCN (Reduced)
TCNModel(
    input_size=1,
    num_channels=[90] * 6,  # Reduced channels and layers
    kernel_size=3,
    dropout=0.33,
    output_size=1
)
# Parameters: ~500k

# ATCN (Reduced)
ATCNModel(
    input_size=1,
    num_channels=[70] * 6,
    kernel_size=3,
    dropout=0.33,
    output_size=1
)
# Parameters: ~500k

# LSTM (Increased)
LSTMModel(
    input_size=1,
    hidden_size=256,
    num_layers=4,
    output_size=1
)
# Parameters: ~500k
```

---

## üîß Recommended Configuration for Fair Comparison

### Target: ~1.0M Parameters Each

```python
CONFIG_FAIR = {
    'TCN': {
        'input_size': 1,
        'num_channels': [128] * 8,
        'kernel_size': 3,
        'dropout': 0.33,
        'output_size': 1
    },
    'ATCN': {
        'input_size': 1,
        'num_channels': [100] * 8,  # Reduced to match TCN params
        'kernel_size': 3,
        'dropout': 0.33,
        'output_size': 1
    },
    'LSTM': {
        'input_size': 1,
        'hidden_size': 356,  # Increased to match TCN params
        'num_layers': 5,     # Increased to match TCN params
        'output_size': 1,
        'dropout': 0.2
    }
}
```

---

## üìù Implementation Changes Needed

### 1. Update `reproduce_paper.py`

```python
# BEFORE (Unfair)
models = {
    'TCN': TCNModel(input_size=1, num_channels=[128]*8, dropout=0.33),
    'ATCN': ATCNModel(input_size=1, num_channels=[128]*8, dropout=0.33),
    'LSTM': LSTMModel(input_size=1, hidden_size=128, num_layers=3)
}

# AFTER (Fair - Parameter-Matched)
models = {
    'TCN': TCNModel(input_size=1, num_channels=[128]*8, dropout=0.33, output_size=1),
    'ATCN': ATCNModel(input_size=1, num_channels=[100]*8, dropout=0.33, output_size=1),
    'LSTM': LSTMModel(input_size=1, hidden_size=356, num_layers=5, output_size=1)
}
```

### 2. Add Parameter Counting Function

```python
def count_parameters(model):
    """Count trainable parameters in a model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# Usage
for name, model in models.items():
    param_count = count_parameters(model)
    print(f"{name}: {param_count:,} parameters ({param_count/1e6:.2f}M)")
```

### 3. Report Parameters in Results

```python
# Add to results JSON
results[appliance_name][model_name]['parameters'] = count_parameters(model)
```

---

## üìä Expected Output (Fair Comparison)

```
Model Parameter Counts:
TCN:  1,048,576 parameters (1.05M)
ATCN: 1,050,000 parameters (1.05M)  ‚Üê Now fair!
LSTM: 1,052,416 parameters (1.05M)  ‚Üê Now fair!
```

---

## ‚ö†Ô∏è Current Bias in Results

### What's Happening Now

1. **ATCN looks artificially better** because it has 40-50% more parameters
2. **LSTM looks artificially worse** because it has only 25% of the parameters
3. **Comparison is scientifically invalid**

### Impact on Metrics

If ATCN has 1.5M params vs LSTM's 265k:
- ATCN will naturally have lower MAE (more capacity)
- ATCN will naturally have higher R¬≤ (more capacity)
- This doesn't mean ATCN architecture is better, just that it's bigger!

---

## üéì Scientific Best Practices

### Option 1: Parameter-Matched Comparison (Recommended)

‚úÖ Match all models to ~1.0M parameters  
‚úÖ Fair comparison of architectural efficiency  
‚úÖ Can claim "architecture X is better"  

### Option 2: Report Parameters Clearly

If you keep current configs:
- ‚ö†Ô∏è Report parameter counts in all tables
- ‚ö†Ô∏è Add disclaimer: "Models not parameter-matched"
- ‚ö†Ô∏è Provide parameter-matched results as supplementary

### Option 3: Multiple Comparisons

Provide both:
1. **Paper's original configs** (for reproducibility)
2. **Parameter-matched configs** (for fair comparison)

---

## üìã Action Items

### Immediate (Before Publishing Results)

- [ ] Add parameter counting to `reproduce_paper.py`
- [ ] Report parameter counts in console output
- [ ] Add parameter counts to results JSON
- [ ] Add disclaimer to README about parameter mismatch

### For Fair Comparison (Recommended)

- [ ] Create `reproduce_paper_fair.py` with matched configs
- [ ] Update model configs to match ~1.0M parameters
- [ ] Re-run experiments with fair configs
- [ ] Compare results: original vs parameter-matched

### Documentation

- [ ] Add parameter analysis to `docs/METHODOLOGY.md`
- [ ] Update `docs/TRAINING.md` with fair configs
- [ ] Add parameter counts to all result tables
- [ ] Explain bias in current comparison

---

## üîç How to Verify Parameter Counts

### Using PyTorch

```python
from torchsummary import summary

# For TCN
model_tcn = TCNModel(input_size=1, num_channels=[128]*8, output_size=1)
summary(model_tcn, input_size=(1, 288, 1))

# For ATCN
model_atcn = ATCNModel(input_size=1, num_channels=[100]*8, output_size=1)
summary(model_atcn, input_size=(1, 288, 1))

# For LSTM
model_lstm = LSTMModel(input_size=1, hidden_size=356, num_layers=5, output_size=1)
summary(model_lstm, input_size=(1, 288, 1))
```

### Manual Count

```python
def count_parameters(model):
    total = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            count = param.numel()
            print(f"{name}: {count:,}")
            total += count
    print(f"\nTotal: {total:,} ({total/1e6:.2f}M)")
    return total
```

---

## üìñ References

This parameter-matching approach follows best practices from:
- "Attention Is All You Need" (Vaswani et al., 2017)
- "An Empirical Evaluation of Generic Convolutional and Recurrent Networks" (Bai et al., 2018)
- Standard ML benchmarking guidelines

---

## ‚úÖ Summary

| Issue | Current State | Recommended Fix |
|-------|---------------|-----------------|
| **Parameter Count** | Mismatched (265k to 1.6M) | Match all to ~1.0M |
| **Fair Comparison** | ‚ùå No | ‚úÖ Yes (after fix) |
| **Scientific Validity** | ‚ö†Ô∏è Questionable | ‚úÖ Valid |
| **Reproducibility** | ‚úÖ Yes | ‚úÖ Yes |

**Bottom Line**: Either match parameters or clearly report the mismatch. Current comparison favors ATCN unfairly.
